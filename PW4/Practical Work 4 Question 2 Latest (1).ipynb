{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from arch import arch_model\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm, t\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kospi = pd.read_csv(\"^kospi_d.csv\")\n",
    "ndx = pd.read_csv(\"^ndx_d.csv\")\n",
    "kospi[\"Return\"] = 100 * (kospi[\"Close\"] - kospi[\"Close\"].shift(1)) / kospi[\"Close\"].shift(1)\n",
    "k = kospi.dropna()\n",
    "ndx[\"Return\"] = 100 * (ndx[\"Close\"] - ndx[\"Close\"].shift(1)) / ndx[\"Close\"].shift(1)\n",
    "n = ndx.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n['Date'] = pd.to_datetime(n['Date'])\n",
    "k['Date'] = pd.to_datetime(k['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare dataset function\n",
    "def load_and_prepare_data(filename):\n",
    "    data = pd.read_csv(filename, parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "    data[\"Return\"] = 100 * (data[\"Close\"] - data[\"Close\"].shift(1)) / data[\"Close\"].shift(1)\n",
    "    return data.dropna()\n",
    "\n",
    "# Load datasets\n",
    "ndx_data = load_and_prepare_data(\"^ndx_d.csv\")\n",
    "kospi_data = load_and_prepare_data(\"^kospi_d.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align datasets to start on the same date\n",
    "# common_start_date = max(ndx_data.index.min(), kospi_data.index.min())\n",
    "# ndx_data, kospi_data = ndx_data.loc[common_start_date:], kospi_data.loc[common_start_date:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 & 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics function\n",
    "def compute_stats(data):\n",
    "    returns = data[\"Return\"]\n",
    "    return {\n",
    "        \"Start Date\": returns.index.min(),\n",
    "        \"End Date\": returns.index.max(),\n",
    "        \"Observations\": len(returns),\n",
    "        \"Mean Return\": returns.mean(),\n",
    "        \"Variance\": returns.var(),\n",
    "        \"Standard Deviation\": returns.std(),\n",
    "        \"Minimum Return\": returns.min(),\n",
    "        \"Maximum Return\": returns.max(),\n",
    "        \"Median Return\": returns.median(),\n",
    "        \"Skewness\": stats.skew(returns),\n",
    "        \"Kurtosis\": stats.kurtosis(returns),\n",
    "    }\n",
    "\n",
    "# Print statistics function\n",
    "def print_stats(name, stats_dict):\n",
    "    print(f\"\"\"\n",
    "{name}\n",
    "- Observations: {stats_dict[\"Observations\"]:,}\n",
    "- Start Date: {stats_dict[\"Start Date\"]:}\n",
    "- End Date: {stats_dict[\"End Date\"]:}\n",
    "- Mean daily return: {stats_dict[\"Mean Return\"]:.3f}\n",
    "- Variance: {stats_dict[\"Variance\"]:.3f}\n",
    "- Standard deviation: {stats_dict[\"Standard Deviation\"]:.5f}\n",
    "- Minimum return: {stats_dict[\"Minimum Return\"]:.3f}%\n",
    "- Maximum return: {stats_dict[\"Maximum Return\"]:.3f}%\n",
    "- Median return: {stats_dict[\"Median Return\"]:.3f}%\n",
    "- Skewness: {stats_dict[\"Skewness\"]:.3f}\n",
    "- Kurtosis: {stats_dict[\"Kurtosis\"]:.3f}\n",
    "\"\"\")\n",
    "\n",
    "# Compute and print statistics\n",
    "ndx_stats, kospi_stats = compute_stats(ndx_data), compute_stats(kospi_data)\n",
    "print_stats(\"NASDAQ-100 (^NDX)\", ndx_stats)\n",
    "print_stats(\"KOSPI (^KOSPI)\", kospi_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean and standard deviation for both indices\n",
    "ndx_mu, ndx_sigma = ndx_data[\"Return\"].mean(), ndx_data[\"Return\"].std()\n",
    "kospi_mu, kospi_sigma = kospi_data[\"Return\"].mean(), kospi_data[\"Return\"].std()\n",
    "\n",
    "# Generate x-values for normal distributions\n",
    "x_values = np.linspace(min(ndx_data[\"Return\"].min(), kospi_data[\"Return\"].min()), \n",
    "                       max(ndx_data[\"Return\"].max(), kospi_data[\"Return\"].max()), 500)\n",
    "\n",
    "# Compute normal density curves\n",
    "ndx_norm = norm.pdf(x_values, ndx_mu, ndx_sigma)\n",
    "kospi_norm = norm.pdf(x_values, kospi_mu, kospi_sigma)\n",
    "\n",
    "# Plot KDE for NDX and KOSPI\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.kdeplot(ndx_data[\"Return\"], color=\"blue\", label=\"NDX KDE\", linewidth=3)\n",
    "sns.kdeplot(kospi_data[\"Return\"], color=\"red\", label=\"KOSPI KDE\", linewidth=3)\n",
    "\n",
    "# Plot fitted normal distributions\n",
    "plt.plot(x_values, ndx_norm, color=\"blue\", linestyle=\"dashed\", label=\"NDX Normal\", linewidth=1)\n",
    "plt.plot(x_values, kospi_norm, color=\"red\", linestyle=\"dashed\", label=\"KOSPI Normal\", linewidth=1)\n",
    "\n",
    "# Labels and legend\n",
    "plt.xlabel(\"Daily Returns\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "# plt.title(\"NDX vs KOSPI Return Density with Normal Overlay\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Plot NASDAQ-100 returns\n",
    "ax.plot(ndx_data.index, ndx_data[\"Return\"], color=\"blue\", alpha=0.5, label=\"NDX\")\n",
    "\n",
    "# Plot KOSPI returns\n",
    "ax.plot(kospi_data.index, kospi_data[\"Return\"], color=\"red\", alpha=0.5, label=\"KOSPI\")\n",
    "\n",
    "# Add horizontal line at 0%\n",
    "ax.axhline(0, linestyle=\"--\", color=\"black\", linewidth=1)\n",
    "\n",
    "# Labels and title\n",
    "# ax.set_title(\"NASDAQ-100 vs. KOSPI Daily Returns (%)\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Return (%)\")\n",
    "\n",
    "# Add legend\n",
    "ax.legend()\n",
    "\n",
    "# Improve layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for VaR\n",
    "var_levels = [0.01, 0.05, 0.10]\n",
    "update_freq = 1  # Daily updates\n",
    "\n",
    "# Function to compute FHS VaR with a rolling 10-year window\n",
    "def filtered_historical_simulation(returns, update_freq, var_levels):\n",
    "    start_date = returns.index.min() + pd.DateOffset(years=10)\n",
    "    dates = returns.loc[start_date:].index  \n",
    "    var_results = {alpha: [] for alpha in var_levels}\n",
    "\n",
    "    for date in dates[::update_freq]:\n",
    "        train_data = returns.loc[date - pd.DateOffset(years=10):date]  # 10-year rolling window\n",
    "\n",
    "        # Fit GARCH(1,1)\n",
    "        garch_model = arch_model(train_data, vol='Garch', p=1, q=1, mean='Constant')\n",
    "        garch_fit = garch_model.fit(disp=\"off\")\n",
    "\n",
    "        # Compute standardized residuals & forecast next period's volatility\n",
    "        residuals = garch_fit.std_resid\n",
    "        forecast = garch_fit.forecast(horizon=1, reindex=False).variance.iloc[-1, 0]\n",
    "        forecast_vol = np.sqrt(forecast)\n",
    "        rolling_mean = garch_fit.params['mu']\n",
    "\n",
    "        # Compute VaR at different levels\n",
    "        for alpha in var_levels:\n",
    "            var_results[alpha].append((date, -rolling_mean - forecast_vol * residuals.quantile(alpha, interpolation='higher')))\n",
    "\n",
    "    return var_results\n",
    "\n",
    "# Compute VaR forecasts\n",
    "ndx_var = filtered_historical_simulation(ndx_data[\"Return\"], update_freq, var_levels)\n",
    "kospi_var = filtered_historical_simulation(kospi_data[\"Return\"], update_freq, var_levels)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "def var_results_to_df(var_results, var_levels):\n",
    "    return pd.DataFrame({f\"VaR {int((1 - alpha) * 100)}%\": [v[1] for v in var_results[alpha]] for alpha in var_levels},\n",
    "                        index=[v[0] for v in var_results[var_levels[0]]])\n",
    "\n",
    "ndx_var_df = var_results_to_df(ndx_var, var_levels)\n",
    "kospi_var_df = var_results_to_df(kospi_var, var_levels)\n",
    "\n",
    "# Export results\n",
    "def save_var_results(df, filename):\n",
    "    filepath = filename if filename not in locals() else f\"new_{filename}\"\n",
    "    df.to_csv(filepath, index_label=\"Date\")\n",
    "    print(f\"VaR results saved to {filepath}\")\n",
    "\n",
    "save_var_results(ndx_var_df, \"ndx_var_results.csv\")\n",
    "save_var_results(kospi_var_df, \"kospi_var_results.csv\")\n",
    "\n",
    "# Plot VaR estimates\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "\n",
    "ndx_var_df.plot(ax=axes[0], title=\"Filtered Historical Simulation VaR for NDX\")\n",
    "axes[0].set_ylabel(\"VaR (%)\")\n",
    "\n",
    "kospi_var_df.plot(ax=axes[1], title=\"Filtered Historical Simulation VaR for KOSPI\")\n",
    "axes[1].set_ylabel(\"VaR (%)\")\n",
    "\n",
    "plt.xlabel(\"Date\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(n['Date'], n['Close'], label=\"NDX Closing Price\", color=\"blue\", alpha=0.7)\n",
    "plt.plot(k['Date'], k['Close'], label=\"KOSPI Closing Price\", color=\"red\", alpha=0.7)\n",
    "\n",
    "# Formatting\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Returns\")\n",
    "# plt.title(\"NDX vs KOSPI Returns Over Time\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalised Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute HITs (VaR violations) and export results\n",
    "def compute_and_export_hits(index_name, returns, var_series_dict, output_filename):\n",
    "    hit_data = {}\n",
    "\n",
    "    # Ensure all data aligns to the same timestamps\n",
    "    common_index = returns.index  # Start with the returns index\n",
    "    for model_name, var_series in var_series_dict.items():\n",
    "        common_index = common_index.intersection(var_series.index)  # Keep only common dates\n",
    "\n",
    "    # Align all series to common dates\n",
    "    returns = returns.loc[common_index]\n",
    "\n",
    "    for model_name, var_series in var_series_dict.items():\n",
    "        var_series = var_series.loc[common_index]  # Align VaR series\n",
    "        var_actual = -var_series  # Convert to negative values for correct interpretation\n",
    "        hit_series = (returns < var_actual).astype(float)  # Compute HITs\n",
    "\n",
    "        # Store only HITs\n",
    "        hit_data[f\"HIT_{model_name}\"] = hit_series\n",
    "\n",
    "    # Convert to DataFrame (ONLY HIT columns)\n",
    "    hit_df = pd.DataFrame(hit_data).dropna()\n",
    "\n",
    "    # Save to CSV (ONLY HITs)\n",
    "    hit_df.to_csv(output_filename, index_label=\"Date\")\n",
    "\n",
    "    # Print HIT rates for each model\n",
    "    for model_name in var_series_dict.keys():\n",
    "        print(f\"{index_name} HIT rate ({model_name}): {hit_df[f'HIT_{model_name}'].mean():.4f}\")\n",
    "\n",
    "    # Compute and print correlation of HITs between models (if multiple models exist)\n",
    "    if len(hit_data) > 1:\n",
    "        hit_correlation = hit_df.corr()\n",
    "        print(f\"Correlation of HITs between models:\\n{hit_correlation}\")\n",
    "\n",
    "    return hit_df\n",
    "\n",
    "# Function to plot VaR violations (HITs)\n",
    "def plot_hits(hit_dfs, title):\n",
    "    plt.figure(figsize=(16, 6))\n",
    "\n",
    "    # Fixed y-axis offsets to maintain clear visibility\n",
    "    offsets = [1.0, 0.97]\n",
    "\n",
    "    # Extract the two datasets\n",
    "    (index_name_1, hit_df_1), (index_name_2, hit_df_2) = hit_dfs.items()\n",
    "    \n",
    "    # Get the first HIT column from each dataset\n",
    "    col_1 = [c for c in hit_df_1.columns if \"HIT_\" in c][0]\n",
    "    col_2 = [c for c in hit_df_2.columns if \"HIT_\" in c][0]\n",
    "\n",
    "    # Prepare the plots with fixed offsets\n",
    "    hit_plot_1 = hit_df_1[col_1].replace(0.0, np.nan).apply(lambda x: offsets[0] if x == 1 else np.nan)\n",
    "    hit_plot_2 = hit_df_2[col_2].replace(0.0, np.nan).apply(lambda x: offsets[1] if x == 1 else np.nan)\n",
    "\n",
    "    # Scatter plot with fixed markers\n",
    "    plt.scatter(hit_plot_1.index, hit_plot_1, marker=\".\", s=15, label=f\"{index_name_1} - {col_1.replace('HIT_', '')}\")\n",
    "    plt.scatter(hit_plot_2.index, hit_plot_2, marker=\".\", s=15, label=f\"{index_name_2} - {col_2.replace('HIT_', '')}\")\n",
    "\n",
    "    plt.ylim(0.95, 1.02)  # Maintain spacing similar to the original\n",
    "    plt.yticks([])  # Remove unnecessary y-axis ticks\n",
    "    plt.ylabel(\"VaR Violations\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.legend(frameon=False)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute HITs for FHS VaR (Filtered Historical Simulation)\n",
    "ndx_hits_fhs = compute_and_export_hits(\"NDX (FHS)\", ndx_data[\"Return\"], {\"VaR 95%\": ndx_var_df[\"VaR 95%\"]}, \"ndx_combined.csv\")\n",
    "kospi_hits_fhs = compute_and_export_hits(\"KOSPI (FHS)\", kospi_data[\"Return\"], {\"VaR 95%\": kospi_var_df[\"VaR 95%\"]}, \"kospi_combined.csv\")\n",
    "\n",
    "# Plot HITs for FHS VaR\n",
    "# \"HIT Plot (VaR Violations) for NDX and KOSPI - FHS\"\n",
    "plot_hits({\"NDX\": ndx_hits_fhs, \"KOSPI\": kospi_hits_fhs}, title=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute likelihood ratio test for each column in hits_data\n",
    "def likelihood_ratio_test(index_name, hits_data, expected_rate=0.05):\n",
    "    for col in hits_data:\n",
    "        hit = hits_data[col]\n",
    "        phat = hit.mean()  # Observed hit rate for this column\n",
    "        llf = stats.bernoulli(phat).logpmf(hit).sum()  # Log-likelihood of observed rate\n",
    "        llf0 = stats.bernoulli(expected_rate).logpmf(hit).sum()  # Log-likelihood under expected rate\n",
    "        lr = 2 * (llf - llf0)  # Likelihood ratio test statistic\n",
    "        pval = 1 - stats.chi2(1).cdf(lr)  # P-value\n",
    "\n",
    "        print(f\"{index_name} LR: {lr:.4f} P-value: {pval:.4f}\")\n",
    "\n",
    "# Apply to datasets\n",
    "likelihood_ratio_test(\"NDX\", ndx_hits_fhs)\n",
    "likelihood_ratio_test(\"KOSPI\", kospi_hits_fhs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Christoffersen's Independence Test for both indices\n",
    "def christoffersen_independence_test(index_name, hits_data):\n",
    "    hits_data_new = hits_data.squeeze()\n",
    "    hit_t = hits_data_new.shift(1)  # Previous day's HIT\n",
    "    hit_tp1 = hits_data_new  # Current day's HIT\n",
    "\n",
    "    # Count transitions\n",
    "    n00 = ((1 - hit_t) * (1 - hit_tp1)).sum()\n",
    "    n10 = (hit_t * (1 - hit_tp1)).sum()\n",
    "    n01 = ((1 - hit_t) * hit_tp1).sum()\n",
    "    n11 = (hit_t * hit_tp1).sum()\n",
    "\n",
    "    # Compute transition probabilities\n",
    "    p00_hat = n00 / (n00 + n01)\n",
    "    p11_hat = n11 / (n11 + n10)\n",
    "    p00 = p00_hat\n",
    "    p11 = p11_hat\n",
    "    # Compute log-likelihood for observed transition probabilities\n",
    "    llf = (\n",
    "        n00 * np.log(p00)\n",
    "        + n10 * np.log(1 - p00)\n",
    "        + n11 * np.log(p11)\n",
    "        + n10 * np.log(1 - p11)\n",
    "    )\n",
    "\n",
    "\n",
    "    # Compute log-likelihood under independence assumption\n",
    "    p11 = 0.05\n",
    "    p00 = 1 - p11\n",
    "    llf0 = (\n",
    "        n00 * np.log(p00)\n",
    "        + n10 * np.log(1 - p00)\n",
    "        + n11 * np.log(p11)\n",
    "        + n10 * np.log(1 - p11)\n",
    "    )\n",
    "\n",
    "    # Compute likelihood ratio test statistic\n",
    "    lr = 2 * (llf - llf0)\n",
    "    pval = 1 - stats.chi2(2).cdf(lr)\n",
    "\n",
    "    print(f\"Christoffersen's Test, {index_name} LR: {lr:.4f} P-value: {pval:.4f}\")\n",
    "\n",
    "# Apply Christoffersen's test to NDX and KOSPI\n",
    "for index_name, hits_data in zip([\"NDX\", \"KOSPI\"], [ndx_hits_fhs, kospi_hits_fhs]):\n",
    "    christoffersen_independence_test(index_name, hits_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform OLS regression and Wald test\n",
    "def ols_regression_with_wald_test(index_name, hits_data, var_df, var_col, expected_violation_rate=0.05):\n",
    "\n",
    "    hit = hits_data - expected_violation_rate  # Adjust violations by subtracting expected rate\n",
    "\n",
    "    # Create lagged HIT variables (5 lags)\n",
    "    lags = pd.concat([hit.shift(i + 1) for i in range(5)], axis=1)\n",
    "\n",
    "    # Select corresponding VaR column\n",
    "    var = var_df[var_col]\n",
    "\n",
    "    # Combine data for regression\n",
    "    data = pd.concat([hit, var, lags], axis=1).dropna()\n",
    "    y = data.iloc[:, 0]  # Dependent variable (adjusted HIT)\n",
    "    x = sm.add_constant(data.iloc[:, 1:])  # Add constant term\n",
    "    x.columns = [\"const\", \"VaR\"] + [f\"hit_L_{i}\" for i in range(1, 6)]  # Rename columns\n",
    "\n",
    "    # Run OLS regression\n",
    "    res = sm.OLS(y, x).fit()\n",
    "\n",
    "    # Wald test for joint significance of all coefficients (except constant)\n",
    "    r = np.eye(len(x.columns))  \n",
    "    joint = res.wald_test(r, scalar=True)\n",
    "    \n",
    "    return {\n",
    "        \"summary\": res.summary(),\n",
    "        \"stat\": f\"Stat: {joint.statistic:.4f}, P-value: {joint.pvalue:.4f}\"\n",
    "    }\n",
    "\n",
    "# Store results for FHS VaR HITs\n",
    "results_fhs = {\n",
    "    \"NDX\": ols_regression_with_wald_test(\"NDX\", ndx_hits_fhs, ndx_var_df, \"VaR 95%\"),\n",
    "    \"KOSPI\": ols_regression_with_wald_test(\"KOSPI\", kospi_hits_fhs, kospi_var_df, \"VaR 95%\"),\n",
    "}\n",
    "\n",
    "# Print results for FHS\n",
    "for index_name in [\"NDX\", \"KOSPI\"]:\n",
    "    print(f\"OLS Regression Summary for {index_name} (FHS):\")\n",
    "    print(results_fhs[index_name][\"summary\"])\n",
    "    print(f\"Wald Test {index_name} (FHS): {results_fhs[index_name]['stat']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform Logit and Probit regression\n",
    "def logit_probit_regression(index_name, hits_data, var_df, var_col):\n",
    "\n",
    "    hit = hits_data  # HIT variable (already 0 or 1)\n",
    "\n",
    "    # Create lagged HIT variables (5 lags)\n",
    "    lags = pd.concat([hit.shift(i + 1) for i in range(5)], axis=1)\n",
    "\n",
    "    # Select corresponding VaR column\n",
    "    var = var_df[var_col]\n",
    "\n",
    "    # Combine data for regression\n",
    "    data = pd.concat([hit, var, lags], axis=1).dropna()\n",
    "    y = data.iloc[:, 0]  # Binary target variable (HIT)\n",
    "    x = sm.add_constant(data.iloc[:, 1:])  # Add intercept\n",
    "    x.columns = [\"const\", \"VaR\"] + [f\"hit_L_{i}\" for i in range(1, 6)]  # Rename columns\n",
    "\n",
    "    # Run Logit and Probit models\n",
    "    logit_model = sm.Logit(y, x).fit(disp=0)\n",
    "    probit_model = sm.Probit(y, x).fit(disp=0)\n",
    "\n",
    "    # Wald test for joint significance of all coefficients (except constant)\n",
    "    r = np.eye(len(x.columns))  \n",
    "    joint_logit = logit_model.wald_test(r, scalar=True)\n",
    "    joint_probit = probit_model.wald_test(r, scalar=True)\n",
    "\n",
    "    return {\n",
    "        \"logit_summary\": logit_model.summary(),\n",
    "        \"logit_stat\": f\"Stat: {joint_logit.statistic:.4f}, P-value: {joint_logit.pvalue:.4f}\",\n",
    "        \"probit_summary\": probit_model.summary(),\n",
    "        \"probit_stat\": f\"Stat: {joint_probit.statistic:.4f}, P-value: {joint_probit.pvalue:.4f}\"\n",
    "    }\n",
    "\n",
    "# Store results for FHS VaR HITs\n",
    "results_fhs_logit_probit = {\n",
    "    \"NDX\": logit_probit_regression(\"NDX\", ndx_hits_fhs, ndx_var_df, \"VaR 95%\"),\n",
    "    \"KOSPI\": logit_probit_regression(\"KOSPI\", kospi_hits_fhs, kospi_var_df, \"VaR 95%\"),\n",
    "}\n",
    "\n",
    "# Print results for FHS\n",
    "for index_name in [\"NDX\", \"KOSPI\"]:\n",
    "    print(f\"Logit Regression Summary for {index_name} (FHS):\")\n",
    "    print(results_fhs_logit_probit[index_name][\"logit_summary\"])\n",
    "    print(f\"\\nProbit Regression Summary for {index_name} (FHS):\")\n",
    "    print(results_fhs_logit_probit[index_name][\"probit_summary\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set decay factor for EWMA (RiskMetrics)\n",
    "lambda_ewma = 0.94  # Daily data\n",
    "\n",
    "# Function to compute RiskMetrics VaR using explicit summation\n",
    "def riskmetrics_var_explicit(data, lambda_ewma, var_levels):\n",
    "    ewma_var = np.zeros(len(data))  # Store variance estimates\n",
    "    \n",
    "    # Compute EWMA variance using explicit summation\n",
    "    for t in range(len(data)):\n",
    "        past_returns = data.iloc[:t+1]  # Take all past data up to time t\n",
    "        weights = (1 - lambda_ewma) * lambda_ewma ** np.arange(len(past_returns))[::-1]  # Compute correct decay weights\n",
    "        ewma_var[t] = np.sum(weights * past_returns**2)  # Apply formula\n",
    "\n",
    "    ewma_vol = np.sqrt(ewma_var)  # Convert variance to standard deviation\n",
    "\n",
    "    # Compute VaR for different confidence levels using normal quantiles\n",
    "    var_results = {alpha: -ewma_vol * stats.norm.ppf(alpha) for alpha in var_levels}  # % VaR\n",
    "\n",
    "    return pd.DataFrame(var_results, index=data.index)\n",
    "\n",
    "# Compute RiskMetrics VaR for NDX and KOSPI using explicit summation\n",
    "ndx_var_riskmetrics_explicit = riskmetrics_var_explicit(ndx_data[\"Return\"].dropna(), lambda_ewma, [0.01, 0.05, 0.10])\n",
    "kospi_var_riskmetrics_explicit = riskmetrics_var_explicit(kospi_data[\"Return\"].dropna(), lambda_ewma, [0.01, 0.05, 0.10])\n",
    "\n",
    "# Find the first date where FHS VaR is available\n",
    "first_ndx_var_date = ndx_var_df.index.min()\n",
    "first_kospi_var_date = kospi_var_df.index.min()\n",
    "\n",
    "# Trim RiskMetrics VaR data to start from the same date as FHS VaR\n",
    "ndx_var_riskmetrics_explicit = ndx_var_riskmetrics_explicit.loc[first_ndx_var_date:]\n",
    "kospi_var_riskmetrics_explicit = kospi_var_riskmetrics_explicit.loc[first_kospi_var_date:]\n",
    "\n",
    "# Plot RiskMetrics VaR\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "\n",
    "# NDX VaR\n",
    "ndx_var_riskmetrics_explicit.plot(ax=axes[0], title=\"RiskMetrics VaR (EWMA) for NDX\")\n",
    "axes[0].set_ylabel(\"VaR (%)\")\n",
    "\n",
    "# KOSPI VaR\n",
    "kospi_var_riskmetrics_explicit.plot(ax=axes[1], title=\"RiskMetrics VaR (EWMA) for KOSPI\")\n",
    "axes[1].set_ylabel(\"VaR (%)\")\n",
    "\n",
    "plt.xlabel(\"Date\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare 5% alpha results of FHS VaR and RiskMetrics VaR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "# Plot 95% VaR for NDX\n",
    "axes[0].plot(ndx_var_df.index, ndx_var_df[\"VaR 95%\"], label=\"FHS VaR 95% (NDX)\", linestyle=\"-\", alpha=0.7)\n",
    "axes[0].plot(ndx_var_riskmetrics_explicit.index, ndx_var_riskmetrics_explicit[0.05], label=\"RiskMetrics VaR 5% (NDX)\", linestyle=\"-\", alpha=0.7)\n",
    "axes[0].set_title(\"Comparison of 95% FHS VaR and 5% RiskMetrics VaR for NDX\")\n",
    "axes[0].set_ylabel(\"VaR (%)\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot 95% VaR for KOSPI\n",
    "axes[1].plot(kospi_var_df.index, kospi_var_df[\"VaR 95%\"], label=\"FHS VaR 95% (KOSPI)\", linestyle=\"-\", alpha=0.7)\n",
    "axes[1].plot(kospi_var_riskmetrics_explicit.index, kospi_var_riskmetrics_explicit[0.05], label=\"RiskMetrics VaR 5% (KOSPI)\", linestyle=\"-\", alpha=0.7)\n",
    "axes[1].set_title(\"Comparison of 95% FHS VaR and 5% RiskMetrics VaR for KOSPI\")\n",
    "axes[1].set_ylabel(\"VaR (%)\")\n",
    "axes[1].set_xlabel(\"Date\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute HITs for RiskMetrics VaR (EWMA model)\n",
    "ndx_hits_riskmetrics = compute_and_export_hits(\"NDX (RiskMetrics)\", ndx_data[\"Return\"], {\"VaR 95%\": ndx_var_riskmetrics_explicit[0.05]}, \"ndx_combined2.csv\")\n",
    "kospi_hits_riskmetrics = compute_and_export_hits(\"KOSPI (RiskMetrics)\", kospi_data[\"Return\"], {\"VaR 95%\": kospi_var_riskmetrics_explicit[0.05]}, \"kospi_combined2.csv\")\n",
    "\n",
    "# Plot HITs for RiskMetrics VaR\n",
    "plot_hits({\"NDX\": ndx_hits_riskmetrics, \"KOSPI\": kospi_hits_riskmetrics}, \"HITs (VaR Violations) for NDX and KOSPI - RiskMetrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply likelihood ratio test for RiskMetrics VaR HITs\n",
    "for index_name, hits_data in zip([\"NDX\", \"KOSPI\"], [ndx_hits_riskmetrics, kospi_hits_riskmetrics]):\n",
    "    likelihood_ratio_test(index_name, hits_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Christoffersen's Independence Test for RiskMetrics VaR HITs\n",
    "for index_name, hits_data in zip([\"NDX\", \"KOSPI\"], [ndx_hits_riskmetrics, kospi_hits_riskmetrics]):\n",
    "    christoffersen_independence_test(index_name, hits_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results for RiskMetrics VaR HITs\n",
    "results_riskmetrics = {\n",
    "    \"NDX\": ols_regression_with_wald_test(\"NDX\", ndx_hits_riskmetrics, ndx_var_riskmetrics_explicit, var_col=0.05),\n",
    "    \"KOSPI\": ols_regression_with_wald_test(\"KOSPI\", kospi_hits_riskmetrics, kospi_var_riskmetrics_explicit, var_col=0.05),\n",
    "}\n",
    "\n",
    "# Print results for RiskMetrics\n",
    "for index_name in [\"NDX\", \"KOSPI\"]:\n",
    "    print(f\"OLS Regression Summary for {index_name} (RiskMetrics):\")\n",
    "    print(results_riskmetrics[index_name][\"summary\"])\n",
    "    print(f\"Wald Test {index_name} (RiskMetrics): {results_riskmetrics[index_name]['stat']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results for RiskMetrics VaR HITs\n",
    "results_riskmetrics_logit_probit = {\n",
    "    \"NDX\": logit_probit_regression(\"NDX\", ndx_hits_riskmetrics, ndx_var_riskmetrics_explicit, var_col=0.05),\n",
    "    \"KOSPI\": logit_probit_regression(\"KOSPI\", kospi_hits_riskmetrics, kospi_var_riskmetrics_explicit, var_col=0.05),\n",
    "}\n",
    "\n",
    "# Print results for RiskMetrics\n",
    "for index_name in [\"NDX\", \"KOSPI\"]:\n",
    "    print(f\"Logit Regression Summary for {index_name} (RiskMetrics):\")\n",
    "    print(results_riskmetrics_logit_probit[index_name][\"logit_summary\"])\n",
    "    print(f\"\\nProbit Regression Summary for {index_name} (RiskMetrics):\")\n",
    "    print(results_riskmetrics_logit_probit[index_name][\"probit_summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tick_loss(returns, var_estimates, alpha=0.05):\n",
    "    returns = returns.reindex(var_estimates.index)\n",
    "    indicator = (returns < var_estimates).astype(int)  # Corrected logic\n",
    "    return alpha * (returns - var_estimates) * (1 - indicator) + (1 - alpha) * (var_estimates - returns) * indicator\n",
    "\n",
    "def diebold_mariano_test(index_name, returns, var_model_1, var_model_2, alpha=0.05):\n",
    "    \"\"\" Diebold-Mariano test using OLS with HAC standard errors \"\"\"\n",
    "\n",
    "    # Convert VaRs to negative values (since VaR represents expected losses)\n",
    "    var_model_1_actual = -var_model_1\n",
    "    var_model_2_actual = -var_model_2\n",
    "\n",
    "    # Compute tick losses\n",
    "    loss_model_1 = tick_loss(returns, var_model_1_actual, alpha)\n",
    "    loss_model_2 = tick_loss(returns, var_model_2_actual, alpha)\n",
    "\n",
    "    # Compute loss differential\n",
    "    d_t = loss_model_1 - loss_model_2\n",
    "    d_t = d_t.dropna()\n",
    "\n",
    "    # Define OLS regression model (d_t ~ constant)\n",
    "    X = np.ones_like(d_t)  # Constant term for regression\n",
    "    ols_model = sm.OLS(d_t, X).fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": int(1.2 * len(d_t) ** (1 / 3)), \"use_correction\": False})  # HAC standard errors\n",
    "\n",
    "    # Extract key statistics\n",
    "    dm_stat = ols_model.params.iloc[0] / ols_model.bse.iloc[0]\n",
    "    p_value = 2 * (1 - stats.norm.cdf(abs(dm_stat)))  # Two-tailed p-value\n",
    "\n",
    "    print(f\"DM Statistics for {index_name}: {dm_stat}, P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align daily returns with VaR data\n",
    "ndx_returns = ndx_data[\"Return\"].loc[ndx_var_riskmetrics_explicit.index]\n",
    "kospi_returns = kospi_data[\"Return\"].loc[kospi_var_riskmetrics_explicit.index]\n",
    "\n",
    "# Align FHS VaR with RiskMetrics VaR for comparison\n",
    "ndx_var_fhs_series = ndx_var_df[\"VaR 95%\"].loc[ndx_var_riskmetrics_explicit.index]\n",
    "kospi_var_fhs_series = kospi_var_df[\"VaR 95%\"].loc[kospi_var_riskmetrics_explicit.index]\n",
    "\n",
    "# Run Diebold-Mariano Test for NDX and KOSPI\n",
    "diebold_mariano_test(\"NDX\", ndx_returns, ndx_var_fhs_series, ndx_var_riskmetrics_explicit[0.05])\n",
    "diebold_mariano_test(\"KOSPI\", kospi_returns, kospi_var_fhs_series, kospi_var_riskmetrics_explicit[0.05])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess KOSPI data\n",
    "kospi = pd.read_csv(\"^kospi_d.csv\", parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "kospi = kospi.sort_index()\n",
    "\n",
    "# Resample to get Thursday closing prices (biweekly non-overlapping returns)\n",
    "thursday_prices = kospi.resample(\"W-THU\").last()\n",
    "non_overlapping_thursdays = thursday_prices.iloc[::2]\n",
    "\n",
    "# Compute non-overlapping returns\n",
    "returns = (non_overlapping_thursdays[\"Close\"] - non_overlapping_thursdays[\"Close\"].shift(1)) / non_overlapping_thursdays[\"Close\"].shift(1) * 100\n",
    "returns = returns.dropna()\n",
    "\n",
    "# Define initial 10-year rolling window\n",
    "start_date = returns.index.min()\n",
    "end_date = start_date + pd.DateOffset(years=10)\n",
    "first_10_years_returns = returns.loc[start_date:end_date]\n",
    "\n",
    "# Fit initial GARCH(1,1) model with a constant mean\n",
    "garch_model = arch_model(first_10_years_returns, vol=\"Garch\", p=1, q=1, mean=\"Constant\")\n",
    "garch_fit = garch_model.fit(disp=\"off\")\n",
    "\n",
    "# Forecast volatility for the next 2-week period\n",
    "forecast = garch_fit.forecast(horizon=1)\n",
    "volatility_forecast = np.sqrt(forecast.variance.iloc[-1].mean())\n",
    "\n",
    "# Estimated mean return\n",
    "estimated_mean = garch_fit.params[\"mu\"]\n",
    "\n",
    "# Compute 5% VaR assuming normal distribution\n",
    "z_score = norm.ppf(0.05)\n",
    "VaR_5_percent_norm = -estimated_mean - z_score * volatility_forecast\n",
    "\n",
    "# Compute 5% VaR assuming Student's t-distribution\n",
    "params = t.fit(first_10_years_returns)\n",
    "degrees_of_freedom = params[0]\n",
    "t_quantile = t.ppf(0.05, df=degrees_of_freedom)\n",
    "VaR_5_percent_t = -estimated_mean - t_quantile * volatility_forecast\n",
    "\n",
    "# Rolling window size\n",
    "window_size = len(first_10_years_returns)\n",
    "\n",
    "# Initialize lists to store VaR values\n",
    "var_5_percent_norm_list, var_5_percent_t_list = [], []\n",
    "\n",
    "# Rolling window estimation\n",
    "for i in range(len(returns) - window_size):\n",
    "    rolling_sample = returns.iloc[i : i + window_size]\n",
    "\n",
    "    # Fit GARCH(1,1) model\n",
    "    garch_model = arch_model(rolling_sample, vol=\"Garch\", p=1, q=1, mean=\"Constant\")\n",
    "    garch_fit = garch_model.fit(disp=\"off\")\n",
    "\n",
    "    # Forecast volatility\n",
    "    forecast = garch_fit.forecast(horizon=1)\n",
    "    volatility_forecast = np.sqrt(forecast.variance.iloc[-1].mean())\n",
    "\n",
    "    # Estimated mean return\n",
    "    estimated_mean = garch_fit.params[\"mu\"]\n",
    "\n",
    "    # Compute VaR for both distributions\n",
    "    VaR_5_p_norm = -estimated_mean - z_score * volatility_forecast\n",
    "    VaR_5_p_t = -estimated_mean - t_quantile * volatility_forecast\n",
    "\n",
    "    var_5_percent_norm_list.append(VaR_5_p_norm)\n",
    "    var_5_percent_t_list.append(VaR_5_p_t)\n",
    "\n",
    "# Convert the VaR lists to Series with timestamps\n",
    "var_norm_series = pd.Series(var_5_percent_norm_list, index=returns.index[window_size:])\n",
    "var_t_series = pd.Series(var_5_percent_t_list, index=returns.index[window_size:])\n",
    "\n",
    "# Plot the two VaR series\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(var_norm_series, label=\"VaR (Normal Dist.)\", linestyle=\"-\", color=\"blue\", alpha=0.7)\n",
    "plt.plot(var_t_series, label=\"VaR (Student's t Dist.)\", linestyle=\"-\", color=\"red\", alpha=0.7)\n",
    "\n",
    "# Add titles and labels\n",
    "# plt.title(\"Rolling 5% 2-Week VaR (Normal vs. Student's t)\", fontsize=14)\n",
    "plt.xlabel(\"Date\", fontsize=12)\n",
    "plt.ylabel(\"Value-at-Risk (%)\", fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"-\", alpha=0.5)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "var_t_series.to_csv(\"var_t_series.csv\", index_label=\"Date\")\n",
    "var_norm_series.to_csv(\"var_normal_series.csv\", index_label=\"Date\")\n",
    "\n",
    "print(\"VaR results saved as var_t_series.csv and var_normal_series.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all datasets have matching timestamps\n",
    "returns = returns.loc[var_norm_series.index]  # Align returns with the VaR series\n",
    "var_t_series = var_t_series.loc[var_norm_series.index]\n",
    "\n",
    "# Run Diebold-Mariano test for Normal VaR vs. t-distributed VaR\n",
    "diebold_mariano_test(\"KOSPI\", returns, var_t_series, var_norm_series, alpha=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7 (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and preprocess biweekly returns\n",
    "kospi_data_new = pd.read_csv(\"^kospi_d.csv\", parse_dates=[\"Date\"], index_col=\"Date\").sort_index()\n",
    "\n",
    "# Resample to get Thursday closing prices (biweekly)\n",
    "thursday_prices = kospi_data_new.resample(\"W-THU\").last()\n",
    "biweekly_prices = thursday_prices.iloc[::2]\n",
    "\n",
    "# Compute biweekly returns and scale by 100\n",
    "biweekly_returns = (biweekly_prices[\"Close\"] - biweekly_prices[\"Close\"].shift(1)) / biweekly_prices[\"Close\"].shift(1) * 100\n",
    "biweekly_returns = biweekly_returns.dropna()  # Remove NaNs from the first row\n",
    "\n",
    "# Compute FHS VaR for 5% level using biweekly returns\n",
    "kospi_var_fhs_biweekly = filtered_historical_simulation(biweekly_returns, update_freq=1, var_levels=[0.05])\n",
    "\n",
    "\n",
    "# Convert dictionary to DataFrame\n",
    "kospi_var_fhs_biweekly_df = pd.DataFrame(kospi_var_fhs_biweekly[0.05], columns=[\"Date\", \"VaR\"]).set_index(\"Date\")\n",
    "\n",
    "# Plot the FHS VaR estimates\n",
    "plt.figure(figsize=(12, 8))\n",
    "kospi_var_fhs_biweekly_df[\"VaR\"].plot(title=\"Filtered Historical Simulation 5% 2-Week VaR for KOSPI\", legend=True)\n",
    "plt.ylabel(\"VaR (%)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all series have the same date index range\n",
    "vaR_combined = pd.concat([var_norm_series, var_t_series, kospi_var_fhs_biweekly_df[\"VaR\"]], axis=1)\n",
    "vaR_combined.columns = [\"VaR (Normal Dist.)\", \"VaR (Student's t Dist.)\", \"VaR (FHS)\"]\n",
    "vaR_combined.dropna(inplace=True)  # Ensure aligned series\n",
    "\n",
    "# Plot the three VaR estimates\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(vaR_combined.index, vaR_combined[\"VaR (Normal Dist.)\"], label=\"VaR (Normal Dist.)\", linestyle=\"-\", color=\"blue\", alpha=0.7)\n",
    "plt.plot(vaR_combined.index, vaR_combined[\"VaR (Student's t Dist.)\"], label=\"VaR (Student's t Dist.)\", linestyle=\"-\", color=\"red\", alpha=0.7)\n",
    "plt.plot(vaR_combined.index, vaR_combined[\"VaR (FHS)\"], label=\"VaR (FHS)\", linestyle=\"-\", color=\"green\", alpha=0.7)\n",
    "\n",
    "# Formatting\n",
    "plt.title(\"Rolling 5% 2-Week VaR Estimates (Normal, t-Distribution, FHS)\", fontsize=14)\n",
    "plt.xlabel(\"Date\", fontsize=12)\n",
    "plt.ylabel(\"Value-at-Risk (%)\", fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align returns with VaR data\n",
    "returns = biweekly_returns.loc[var_norm_series.index]  \n",
    "var_fhs_series = kospi_var_fhs_biweekly_df[\"VaR\"].loc[var_norm_series.index]  # Use correct FHS VaR\n",
    "\n",
    "# Run the Diebold-Mariano test\n",
    "diebold_mariano_test(\"KOSPI (Biweekly)\", returns, var_fhs_series, var_norm_series, alpha=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure datasets have matching timestamps\n",
    "returns = biweekly_returns.loc[var_norm_series.index]\n",
    "var_fhs_series = kospi_var_fhs_biweekly_df[\"VaR\"].loc[var_norm_series.index]\n",
    "\n",
    "# Compute HITs for KOSPI\n",
    "kospi_hits_norm_df = compute_and_export_hits(\"KOSPI (Biweekly)\", returns, {\"VaR_Normal\": var_norm_series}, \"kospi_hits_norm.csv\")\n",
    "kospi_hits_fhs_df = compute_and_export_hits(\"KOSPI (Biweekly)\", returns, {\"VaR_FHS\": var_fhs_series}, \"kospi_hits_fhs.csv\")\n",
    "\n",
    "# Plot HITs\n",
    "plot_hits({\"KOSPI Norm\": kospi_hits_norm_df, \"KOSPI FHS\": kospi_hits_fhs_df}, \"HITs for KOSPI - Normal vs. FHS VaR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply likelihood ratio test for 2-week VaR HITs\n",
    "likelihood_ratio_test(\"KOSPI\", kospi_hits_norm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Christoffersen's Independence Test for 2-week VaR HITs\n",
    "christoffersen_independence_test(\"KOSPI\", kospi_hits_norm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FHS VaR results (First Approach)\n",
    "kospi_var_df = pd.read_csv(\"kospi_var_results.csv\", parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "\n",
    "# Extract the 5% VaR (which corresponds to 95% confidence level in one-tailed tests)\n",
    "kospi_var_5p_first = kospi_var_df[\"VaR 95%\"] * np.sqrt(10)\n",
    "\n",
    "# Load Gaussian VaR results (Second Approach)\n",
    "var_norm_series = pd.read_csv(\"var_normal_series.csv\", parse_dates=[\"Date\"], index_col=\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store standard GMZ results for 2-week GARCH VaR HITs\n",
    "results_2_week_garch = {\n",
    "    \"KOSPI\": ols_regression_with_wald_test(\"KOSPI\", kospi_hits_norm_df, var_norm_series, var_col=\"0\"),\n",
    "}\n",
    "\n",
    "# Print results for RiskMetrics\n",
    "print(f\"OLS Regression Summary for {\"KOSPI\"} (2-week GARCH VaR):\")\n",
    "print(results_2_week_garch[\"KOSPI\"][\"summary\"])\n",
    "print(f\"Wald Test {\"KOSPI\"} (2-week GARCH VaR): {results_2_week_garch[\"KOSPI\"]['stat']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results for RiskMetrics VaR HITs\n",
    "results_garch_logit_probit = {\n",
    "    \"KOSPI\": logit_probit_regression(\"KOSPI\", kospi_hits_norm_df, var_norm_series, var_col=\"0\"),\n",
    "}\n",
    "\n",
    "# Print results for RiskMetrics\n",
    "print(f\"Logit Regression Summary for {\"KOSPI\"} (2-week GARCH VaR):\")\n",
    "print(results_garch_logit_probit[\"KOSPI\"][\"logit_summary\"])\n",
    "print(f\"Wald Test {\"Logit\"} (2-week GARCH VaR): {results_garch_logit_probit[\"KOSPI\"]['logit_stat']}\\n\")\n",
    "print(f\"\\nProbit Regression Summary for {\"KOSPI\"} (2-week GARCH VaR):\")\n",
    "print(results_garch_logit_probit[\"KOSPI\"][\"probit_summary\"])\n",
    "print(f\"Wald Test {\"Probit\"} (2-week GARCH VaR): {results_garch_logit_probit[\"KOSPI\"]['probit_stat']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7 (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Student's t-distribution VaR results (Second Approach)\n",
    "# var_t_series = pd.read_csv(\"var_t_series.csv\", parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "\n",
    "# Find the common start date\n",
    "common_start_date = max(kospi_var_5p_first.index.min(), var_norm_series.index.min())\n",
    "\n",
    "# Trim both series to start from the common date\n",
    "kospi_var_5p_first_aligned = kospi_var_5p_first.loc[common_start_date:]\n",
    "var_norm_series = var_norm_series.loc[common_start_date:]\n",
    "\n",
    "# Ensure alignment of both series' indices\n",
    "kospi_var_5p_first_aligned = kospi_var_5p_first_aligned.reindex(var_norm_series.index, method=\"nearest\")\n",
    "\n",
    "# Plot the aligned VaR estimates\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# First Approach: Filtered Historical Simulation VaR\n",
    "plt.plot(kospi_var_5p_first_aligned, label=\"FHS VaR (First Approach)\", linestyle=\"-\", color=\"blue\", alpha=0.7)\n",
    "\n",
    "# Second Approach: Student’s t-distribution VaR\n",
    "plt.plot(var_norm_series, label=\"Normal VaR (Second Approach)\", linestyle=\"-\", color=\"red\", alpha=0.7)\n",
    "\n",
    "# Second Approach: Student’s t-distribution VaR\n",
    "# plt.plot(var_t_series, label=\"Student's t VaR (Second Approach)\", linestyle=\"-\", color=\"red\", alpha=0.7)\n",
    "\n",
    "# Labels and title\n",
    "# plt.title(\"Comparison of 5% VaR Estimates for KOSPI (FHS vs. Normal)\", fontsize=14)\n",
    "plt.xlabel(\"Date\", fontsize=12)\n",
    "plt.ylabel(\"Value-at-Risk (%)\", fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"-\", alpha=0.5)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the common start date\n",
    "common_start_date = max(kospi_var_5p_first.index.min(), var_norm_series.index.min())\n",
    "\n",
    "# Trim both series to start from the common date\n",
    "kospi_var_5p_first_aligned = kospi_var_5p_first.loc[common_start_date:]\n",
    "var_t_series = var_norm_series.loc[common_start_date:]\n",
    "returns = returns.loc[common_start_date:]\n",
    "\n",
    "print(returns)\n",
    "print(kospi_var_5p_first_aligned)\n",
    "print(var_t_series)\n",
    "\n",
    "var_t_series = var_t_series.squeeze()\n",
    "\n",
    "# Run Diebold-Mariano test for VaR estimates\n",
    "diebold_mariano_test(\"KOSPI\", returns, kospi_var_5p_first_aligned, var_t_series, alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure datasets have matching timestamps\n",
    "returns = biweekly_returns.loc[var_norm_series.index]\n",
    "var_fhs_series = kospi_var_fhs_biweekly_df[\"VaR\"].loc[var_norm_series.index]\n",
    "\n",
    "# Compute HITs for KOSPI\n",
    "kospi_hits_norm_df = compute_and_export_hits(\"KOSPI (Biweekly)\", returns, {\"VaR_Normal\": var_norm_series['0']}, \"kospi_hits_norm.csv\")\n",
    "kospi_hits_fhs_df = compute_and_export_hits(\"KOSPI (Biweekly)\", returns, {\"VaR_FHS\": var_fhs_series}, \"kospi_hits_fhs.csv\")\n",
    "\n",
    "# Plot HITs\n",
    "plot_hits({\"KOSPI Norm\": kospi_hits_norm_df, \"KOSPI FHS\": kospi_hits_fhs_df}, \"HITs for KOSPI - Normal vs. FHS VaR\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
