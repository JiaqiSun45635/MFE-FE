{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: #0058bd;\">Model Selection and Cross-Validation</span>\n",
    "\n",
    "\n",
    "### <span style=\"color: #0058bd;\">Exercise 35</span>\n",
    "Four portfolios we have been looking at, and considering all 8 sets of\n",
    "regressors which range from no factor to all 3 factors, which model is preferred\n",
    "by AIC, BIC, GtS and StG?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-28T12:33:47.160722Z",
     "iopub.status.busy": "2023-09-28T12:33:47.160722Z",
     "iopub.status.idle": "2023-09-28T12:33:49.590078Z",
     "shell.execute_reply": "2023-09-28T12:33:49.589071Z"
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Reading Fama-French dataset, exporting factors and portfolios (see 6_exercises_regression_basics.py for details and explanations)\n",
    "ff = pd.read_hdf(\"../data/ff-pdr.h5\", \"ff\")\n",
    "factors = sm.add_constant(ff.iloc[:, :3])\n",
    "portfolios = ff.iloc[:, 4:]\n",
    "portfolios = portfolios[[\"SMALL LoBM\", \"SMALL HiBM\", \"BIG LoBM\", \"BIG HiBM\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color: #0058bd;\">Our Approach: Criteria for Model Selection</span>\n",
    "\n",
    "We will run multiple regression to determine the optimal model for explaining portfolio returns, using statistical criteria like the AIC (Akaike Information Criterion) and the BIC (Bayesian Information Criterion). \n",
    "\n",
    "**<span style=\"color: #0058bd;\">Akaike Information Criterion:</span>** The criterion is derived from the concept of information entropy and wants to find the model that minimizes the loss of information. The formula is \n",
    "$$\n",
    "    \\text{AIC} = -2 \\cdot \\ln(\\mathcal{L}) + 2k,\n",
    "$$\n",
    "where $\\ln\\left(\\mathcal{L}\\right)$ is the natural log of the likelihood of the model (indicating goodness of fit), and $k$ is the number of parameters in the model (penalty for complexity). A *lower AIC indicates a better model*. \n",
    "\n",
    "**<span style=\"color: #0058bd;\">Bayesian Information Criterion:</span>** The criterion is derived from the concept of selecting the most probable model given the data (Bayesian framework). Compared to the AIC, the BIC *adds a more substantial penalty* for model complexity as the penalty for complexity grows with the logarithm of $N$. The formula is\n",
    "$$\n",
    "    \\text{BIC} = -2 \\cdot \\ln(\\mathcal{L}) + k \\cdot \\ln(N),\n",
    "$$\n",
    "where $N$ is the number of observations. Similar to the AIC, a *lower BIC suggests a better model*. \n",
    "\n",
    "**<span style=\"color: #0058bd;\">A Philosophical Detour on the Differences:</span>** While both look similar, they are trying to answer two distinct questions. The AIC tries to select a model that is the closest description of an unknown, probably very high dimensional, truth (it assumes that all models are only approximations to a very high-dimensional truth). However, the truth is not in the set of candidate models that we consider. On the other hand, the BIC starts from a set of candidate models, and tries to find the true model among the set of candidate models. \n",
    "\n",
    "**<span style=\"color: #0058bd;\">Practical Consideration:</span>** While not universally accepted, it is often a well-meant advice to report both AIC and BIC in your work. In many cases, they will agree on model selection. If they do not, specify the discrepancy across models. \n",
    "\n",
    "**<span style=\"color: #0058bd;\">Application to our Context:</span>** In a linear regression model $Y = X\\beta + \\epsilon$ with $\\epsilon_i \\sim N(0, \\sigma^2)$ both AIC and BIC use the likelihood function $\\mathcal{L}(\\beta, \\sigma^2|Y)$. With normally distributed errors, the likelihood is related to the estimated variance of residuals $\\hat{\\sigma}^2$ and the RSS. One can show that normalized versions of AIC and BIC are\n",
    "$$\n",
    "    \\text{AIC} = \\ln \\left(\\hat{\\sigma}^2\\right) + 2 \\cdot \\frac{k}{N} ~~~~\\text{ and }~~~~\\text{BIC} = \\ln \\left(\\hat{\\sigma}^2 \\right) + k \\cdot \\frac{\\ln(N)}{N}.\n",
    "$$ \n",
    "Smaller residual error means a better fit, and hence AIC and BIC will be lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_false = [True, False]\n",
    "params = [true_false] * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We use itertools.product to get all possible combinations of True and False for the three factors\n",
    "# product() function computes the Cartesian product of input iterables. It requires each iterable as a separate argument\n",
    "# *params unpacks the list of lists into individual arguments for product()\n",
    "choices = list(product(*params))\n",
    "print(f\"Number of different models: {len(choices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-28T12:33:49.593079Z",
     "iopub.status.busy": "2023-09-28T12:33:49.592080Z",
     "iopub.status.idle": "2023-09-28T12:33:49.662105Z",
     "shell.execute_reply": "2023-09-28T12:33:49.662105Z"
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We iterate over each portfolio return column and treat it as the dependent variable in the regression\n",
    "for column in portfolios:\n",
    "    # Initialize the dictionary to store the results\n",
    "    results = {}\n",
    "\n",
    "    for i in range(len(choices)):\n",
    "        # Each regression includes a subset of factors (indicated by sel)\n",
    "        sel = [True] + list(choices[i]) # always include the intercept\n",
    "        x = factors.loc[:, sel]\n",
    "        \n",
    "        # Use list comprehension to initialize the portfolio names\n",
    "        names = tuple(name for name in x.columns)\n",
    "        res = sm.OLS(portfolios[column], x).fit()\n",
    "\n",
    "        # Recover AIC and BIC from the regression results and store in results dictionary\n",
    "        results[names] = [res.aic, res.bic]\n",
    "    # Store the results in a DataFrame (for each combination of factors)\n",
    "    ic = pd.DataFrame(results, index=[\"AIC\", \"BIC\"]).T\n",
    "\n",
    "    # Recover the model with the lowest AIC and BIC\n",
    "    aic_model = ic.sort_values(\"AIC\").index[0]\n",
    "    bic_model = ic.sort_values(\"BIC\").index[0]\n",
    "\n",
    "    # For each portfolio, print the best model according to AIC and BIC\n",
    "    print(f\"For {column}, AIC selects {aic_model}\")\n",
    "    print(f\"For {column}, BIC selects {bic_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #0058bd;\">From General to Specific (GtS)</span>\n",
    "\n",
    "For each of the portfolios, we start with a list of included variables that includes all three factors. We can then use a loop to see if any of the included variables have insignificant t-stats.  We first create a temporary set of regressors that uses the factors are are in `included`. We can then check if any of the t-stats are less than our critical value that is defined above.  If one is less than the value, we need to drop the variable. We sort the absolute t-stats so that the minimum is first, and then get the variable name from the index. Finally, we use `.remove` to remove this name from the list of included variables. \n",
    "\n",
    "If no t-stat is less than our critical value, we can call `break` which  terminates the loop early. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute critical value from the normal distribution at 99.5% confidence level\n",
    "cv = stats.norm.ppf(0.995); cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a DataFrame to store the final portfolio-factor combinations\n",
    "final_pf_factors = pd.DataFrame(columns=[\"Portfolio\", \"Included Variables (GtS)\", \"Included Variables (StG)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pf_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in portfolios: \n",
    "    included = list(factors.columns)\n",
    "    y = portfolios[column]\n",
    "    for i in range(3): \n",
    "        x = factors.loc[:, included]\n",
    "        # Run OLS regression\n",
    "        res = sm.OLS(y, x).fit(cov_type=\"HC0\")\n",
    "        # Recover t-values from the regressions\n",
    "        tstats = res.tvalues\n",
    "        # Test for significance of the lowest t-value\n",
    "        if tstats.abs().min() < cv: \n",
    "            # If there is a t-value that is not significant, remove the minimum value\n",
    "            sorted_tstats = tstats.abs().sort_values()\n",
    "            remove = sorted_tstats.index[0]\n",
    "            print(f\"Portfolio {column} | Iteration {i+1}: Remove {remove} from the model\")\n",
    "            included.remove(remove)\n",
    "        else:\n",
    "            print(f\"Portfolio {column} | Iteration {i+1}: Nothing to remove\")\n",
    "            break\n",
    "    print(f\"For {column}, GtS selects {included}\")\n",
    "    # Store the final set of factors for the given portfolio\n",
    "    final_pf_factors = pd.concat(\n",
    "        [final_pf_factors, pd.DataFrame({\"Portfolio\": [column], \"Included Variables (GtS)\": [included], \"Included Variables (StG)\": [[]]})], ignore_index=True\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pf_factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #0058bd;\">From Specific to General (StG)</span>\n",
    "\n",
    "Instead of GtS, we can reverse the process and go from Specific to General (StG). For each of the portfolios, we start with an empty list of variables `included` and a list of excluded variables `excluded` that contains all three factors/regressors. \n",
    "\n",
    "We then successively add one of the excluded regressors, one at a time, and run an OLS regression with the current selection to obtain the t-stats. If the t-stat is larger than our critical value, we include the variable in `included`. We then continue the process and check whether we can add another of the factors in `excluded` to `included` by checking the t-stats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-28T12:33:49.662105Z",
     "iopub.status.busy": "2023-09-28T12:33:49.662105Z",
     "iopub.status.idle": "2023-09-28T12:33:49.722404Z",
     "shell.execute_reply": "2023-09-28T12:33:49.722404Z"
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for column in portfolios:\n",
    "    # Store variables that pass the significance test\n",
    "    included = []\n",
    "    # Initially, all factors all excluded\n",
    "    excluded = factors.columns\n",
    "    # Portfolio returns for the current iteration\n",
    "    y = portfolios[column]\n",
    "    # Check which of the excluded factors can be added to the regression?\n",
    "    for i in range(len(excluded)):\n",
    "        tstats = {}\n",
    "        # Iterate over all excluded variables\n",
    "        for next_var in excluded:\n",
    "            # Currently included and the next variable that we are considering to add\n",
    "            col_names = included + [next_var]\n",
    "            x = factors.loc[:, col_names]\n",
    "            # Fit the regression\n",
    "            res = sm.OLS(y, x).fit(cov_type=\"HC0\")\n",
    "            tstats[next_var] = res.tvalues.iloc[-1]\n",
    "        tstats = pd.Series(tstats)\n",
    "        # Check whether the new added variable is significant\n",
    "        if tstats.abs().max() > cv:\n",
    "            sorted_tstats = tstats.abs().sort_values()\n",
    "            # Take the t-stat value from the candidate variable (the last one added)\n",
    "            included = included + [sorted_tstats.index[-1]]\n",
    "        else:\n",
    "            break\n",
    "        # Update the list of excluded variables (remove the last one added)\n",
    "        excluded = set(factors.columns).difference(included)\n",
    "    print(f\"For {column}, StG selects {included}\")\n",
    "     # Update the GtS DataFrame with the included variables from StG procedure\n",
    "    final_pf_factors.loc[final_pf_factors[\"Portfolio\"] == column, \"Included Variables (StG)\"] = [included]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pf_factors[\"Same Variables?\"] = final_pf_factors.apply(\n",
    "    # .apply() applies a function (or lambda function) to each row (axis = 1) of the DataFrame\n",
    "    # For each row of the DataFrame, the lambda function is called\n",
    "    # Each row of the DataFrame is passed as a Pandas Series to the lambda function\n",
    "    lambda row: set(row[\"Included Variables (GtS)\"]) == set(row[\"Included Variables (StG)\"]), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pf_factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #0058bd;\">Exercise 36</span>\n",
    "\n",
    "Cross-validation is a method of analyzing the **in-sample forecasting ability** of a\n",
    "cross-sectional model by using $\\alpha\\%$ of the data to estimate the model and\n",
    "then measuring the fit using the remaining $(100-\\alpha)\\%$. The most common forms\n",
    "are 5- and 10-fold cross-validation which use $\\alpha=20\\%$ and $10\\%$, respectively.\n",
    "k-fold cross validation is implemented by randomly grouping the data into\n",
    "k-equally-sized groups, using k-1 of the groups to estimate parameters, and\n",
    "then evaluating using the bin that was held out. This is then repeated so that\n",
    "each bin is held out.\n",
    "\n",
    "1. Implement cross-validation using the 5- and 10-fold methods for all 8 models.\n",
    "2. For each model, compute the full-sample sum of squared errors as well as the\n",
    "   sum-of-squared errors using the held-out sample. Note that all data points\n",
    "   will appear exactly once in both of these sum or squared errors. What happens\n",
    "   to the cross-validated $R^{2}$ when computed on the held out sample when compared\n",
    "   to the full-sample $R^{2}$? (k-fold cross validated SSE by the TSS).\n",
    "\n",
    "\n",
    "##### <span style=\"color: #0058bd;\">An Aside: Relationship Between MSE, SSE, and R-squared</span>\n",
    "\n",
    "Remember that the Sum of Squared Errors (SSE) and the Total Sum of Squares (TTS) are given by \n",
    "$$\n",
    "   \\text{SSE} = \\sum_{i=1}^N \\left(y_i - \\hat{y}_i\\right)^2 ~~~~~\\text{and}~~~~~\\text{TSS} = \\sum_{i=1}^N \\left(y_i - \\bar{y}\\right)^2.\n",
    "$$\n",
    "\n",
    "Mean Squared Errors (MSE) are simply the normalized versions\n",
    "$$\n",
    "   \\text{MSE}_{\\text{resid}} = \\frac{\\text{SSE}}{N} ~~~~~\\text{and}~~~~~\\text{MSE}_{\\text{total}} = \\frac{\\text{TSS}}{N}.\n",
    "$$\n",
    "Hence, $R^2$ in terms of MSE is given by\n",
    "$$\n",
    "   R^2 = 1 - \\frac{\\text{SSE}}{\\text{TSS}}  = 1 - \\frac{\\text{MSE}_{\\text{resid}}}{\\text{MSE}_{\\text{total}}}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = np.random.RandomState(2309)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize number of folds\n",
    "folds = 10\n",
    "# Retrieve number of observations\n",
    "nobs = portfolios.shape[0]\n",
    "# Re-shuffle all observations\n",
    "order = list(rs.permutation(nobs))\n",
    "block = nobs / folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-28T12:33:49.748547Z",
     "iopub.status.busy": "2023-09-28T12:33:49.748547Z",
     "iopub.status.idle": "2023-09-28T12:33:50.268702Z",
     "shell.execute_reply": "2023-09-28T12:33:50.268702Z"
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for column in portfolios:\n",
    "    portfolio = portfolios[column]\n",
    "    model_errors = {}\n",
    "\n",
    "    # Loop over different feature combinations\n",
    "    for j in range(len(choices)):\n",
    "        # We always want to include a constant\n",
    "        sel = [True] + list(choices[j])\n",
    "        model_factors = factors.loc[:, sel]\n",
    "        # Same shape as our y-variable\n",
    "        errors = portfolio.copy() \n",
    "        # START: Cross-validation part\n",
    "        for i in range(folds):\n",
    "            # Include all observations except for the i-th block (order was randomly shuffled)\n",
    "            include = order[: int(i * block)] + order[int((i + 1) * block) :]\n",
    "            # The i-th block is our hold-out sample\n",
    "            hold_out = order[int(i * block) : int((i + 1) * block)]\n",
    "            y = portfolio.iloc[include]\n",
    "            x = model_factors.iloc[include]\n",
    "            res = sm.OLS(y, x).fit()\n",
    "            # Predict for the hold-out fold\n",
    "            y_hat = res.predict(model_factors.iloc[hold_out])\n",
    "            # Compute the prediction errors\n",
    "            err = portfolio.iloc[hold_out] - y_hat\n",
    "            errors.loc[err.index] = err\n",
    "        model_name = tuple(factors.columns[sel])\n",
    "        model_errors[model_name] = errors\n",
    "    # Save model errors under the specific model name and convert from dictionary into DataFrame\n",
    "    model_errors = pd.DataFrame(model_errors)\n",
    "    # Compute the mean squared error (MSE) for each model and sort them\n",
    "    mse = (model_errors**2).mean()\n",
    "    mse = mse.sort_values()\n",
    "\n",
    "    # Select the factors (index) of the best model according to the MSE and drop the NAs (if any)\n",
    "    selected_factors = pd.Series(mse.index[0]).dropna()\n",
    "    # Select the best model obtained through cross-validation\n",
    "    x = factors[selected_factors]\n",
    "    # Estimate the \"best model\" on the full dataset\n",
    "    res = sm.OLS(portfolios[column], x).fit()\n",
    "    print(f\"\\n For {column}, CV selects {tuple(selected_factors)}\")\n",
    "    print(\n",
    "        f\"The MSEs are {mse.iloc[0]} (CV) and {res.mse_resid} (full sample)\"\n",
    "    )\n",
    "    # MSE_total will be the same for CV or full sample because \"all data points will appear exactly once in both of squared errors\"\n",
    "    print(\n",
    "        f\"The R2s are {1 - mse.iloc[0] / res.mse_total} (CV) and {1 - res.mse_resid / res.mse_total} (full sample)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** MSE is always larger in the CV than for the full sample. Similarly, $R^2$ is always lower in CV than in the full sample. Why? \n",
    "\n",
    "This is by construction. When using the full sample, the model is fitted using the entire dataset. The model is optimized to minimize residuals (SSE) for *all data points* because the same data is used for both training and testing. However, using CV, the model is evaluated on unseen data (test folds). More specifically, the model is trained on $k-1$ folds, but evaluated on the $k$-th fold, which was not part of the training. The model will not perform as well on the hold-out fold because it has never seen this data before. Hence, cross-validation penalizes overfitting because it tests the model on unseen data, where the model cannot explain noise or overfit to \"seemingly irrelevant\" details of the data. \n",
    "\n",
    "**Summary:** $\\text{SSE}_{\\text{CV}} \\geq \\text{SSE}_{\\text{full}}$ and hence $R^2_{\\text{CV}} \\leq R^2_{\\text{full}}$. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
